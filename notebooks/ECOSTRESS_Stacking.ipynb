{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xa\n",
    "from pathlib import Path\n",
    "import src.data.ecostress_io as eio\n",
    "import rioxarray\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_partitions = 8\n",
    "root_path = Path(\"/mnt/ecostress/rhone-ecostress-data/\")\n",
    "\n",
    "with open(f\"{root_path}/geo-countries/archive/countries.geojson\", \"rb\") as f:\n",
    "    all_countries_geojson = json.loads(f.read())\n",
    "\n",
    "\n",
    "for i in all_countries_geojson['features']:\n",
    "    if i['properties']['ADMIN'] == \"France\":\n",
    "        france_geo = i\n",
    "\n",
    "\n",
    "france_gdf = gpd.GeoDataFrame.from_features([france_geo]).explode()\n",
    "\n",
    "france_gdf[france_gdf.area==max(france_gdf.area)].plot()\n",
    "\n",
    "aoi = france_gdf[france_gdf.area==max(france_gdf.area)]\n",
    "\n",
    "aoi_geojson = json.loads(aoi.to_json())\n",
    "\n",
    "xmin, ymin, xmax, ymax = aoi.total_bounds\n",
    "bounds_tuple = (4, 42, 7, 47)\n",
    "xmin, ymin, xmax, ymax = bounds_tuple  # hardcoding since concattenating 1000s of ecostress files with different overlaps hangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global rivers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers_df = gpd.read_file(Path(root_path, \"europe_rivers/eu_river.shp\"))\n",
    "\n",
    "rivers_df['R_ID'] = rivers_df['R_ID'].apply(int).apply(str)\n",
    "\n",
    "france_rivers_df = rivers_df.cx[xmin:xmax, ymin:ymax]\n",
    "\n",
    "aoi.crs = france_rivers_df.crs # setting crs for aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to make sure they overlay, geopandas has poor support for linestrings so we subset by the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = aoi.plot(color=\"grey\", edgecolor=\"black\")\n",
    "\n",
    "france_rivers_df.plot(ax=base, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading in paths and example data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_path = Path(root_path, \"ECO3ANCQA\")\n",
    "et_path = Path(root_path, \"ECO3ETPTJPL\")\n",
    "esi_path = Path(root_path, \"ECO4ESIPTJPL\")\n",
    "\n",
    "whole_tif_qa_paths, csv_qa_paths, xml_qa_paths = eio.separate_extensions(qa_path)\n",
    "\n",
    "whole_tif_etdaily_paths, csv_et_paths, xml_et_paths = eio.separate_extensions(et_path, \"*ETdaily*.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipping all et tifs, takes about 10 min with threading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = Path(\"/home/ryan/work/tmp\")\n",
    "\n",
    "clipped_scene_paths = [Path(p) for p in tempdir.glob(\"*clipped*\")]\n",
    "\n",
    "if clipped_scene_paths == []:\n",
    "    client = Client()\n",
    "    \n",
    "    batches = eio.batches_from(whole_tif_etdaily_paths, 8)\n",
    "    \n",
    "    batch_results = []\n",
    "    \n",
    "    for batch in batches:\n",
    "    \n",
    "        batch_result = dask.delayed(eio.clip_and_save)(batch, bounds_tuple, filter_nan = True, outDir=tempdir)\n",
    "        batch_results.append(batch_result)\n",
    "        \n",
    "    result_futures = client.compute(batch_results, scheduler='processes')\n",
    "\n",
    "    clipped_scene_paths = [i.result() for i in result_futures if i.result()]# gets rid of None that denotes too little scene overlap\n",
    "\n",
    "    client.restart()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "france_river_lines = france_rivers_df.copy()\n",
    "buffered_france_rivers_df = france_rivers_df.to_crs(epsg=2154)\\\n",
    "                                            .buffer(5000)\\\n",
    "                                            .to_crs(epsg=4326) # buffers by 5000 meters\n",
    "france_rivers_df['geometry'] = buffered_france_rivers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = eio.read_ecostress_scene(clipped_scene_paths[0]).rio.resolution()\n",
    "crs = france_rivers_df.crs\n",
    "aoi_grid = eio.gdf_to_dataarray(france_rivers_df, crs, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_scene_paths = [Path(p) for p in tempdir.glob(\"*resampled*\")]\n",
    "\n",
    "if resampled_scene_paths == []:\n",
    "    client = Client()\n",
    "    batches = eio.batches_from(resampled_scene_paths, n_partitions)\n",
    "\n",
    "    def wrapper(paths, aoi_grid, tempdir, path_id):\n",
    "        return_paths = []\n",
    "        for path in paths:\n",
    "            x = eio.read_mask_ecostress_scene(path)\n",
    "            y = eio.resample_xarray_to_basis(x, aoi_grid)\n",
    "            return_paths.append(eio.write_tmp(y, tempdir, path_id))\n",
    "        return return_paths\n",
    "\n",
    "    all_results = []\n",
    "    for batch in batches:\n",
    "        sub_result = dask.delayed(wrapper)(batch, aoi_grid, tempdir, \"resampled\")\n",
    "        all_results.append(sub_result)\n",
    "        \n",
    "    result_future = client.compute(all_results, scheduler=\"processes\")\n",
    "    \n",
    "    resampled_scene_paths = [i.result() for i in result_future]\n",
    "    \n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data_arrays = eio.read_scenes(resampled_scene_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_tseries = xa.concat(resampled_data_arrays, dim=\"date\").sortby('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geocube.api.core import make_geocube\n",
    "france_rivers_df['value'] = 1 # allow sus to make non empty dataset, required for resampling\n",
    "river_arr = make_geocube(vector_data=france_rivers_df, resolution=resolution)['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_path = Path(root_path, \"era5-download.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_dataset = xa.open_dataset(reanalysis_path, chunks = {\"time\": 1, \"latitude\": 39, \"longitude\": 41})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_dataset['vpd'] = eio.vapor_deficit(met_dataset['t2m']-273.15,met_dataset['d2m']-273.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_dataset = met_dataset.rio.set_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd = met_dataset['vpd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd = vpd.rio.set_crs(4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reduce hourly vpd before reprojecting because too much memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "local time in utc is 1 hour behind paris, so I take the mean of vpd over 10am-3pm local time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_mask = np.isin(vpd.time.dt.hour, [9, 10, 11, 12, 13, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_vpd = vpd.isel(time=daytime_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_vpd_daily = daytime_vpd.resample(time=\"1D\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_vpd_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need this much mem to reproject..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float32(1).itemsize * np.prod([395, 6101, 6558]) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.enums import Resampling\n",
    "daytime_vpd_daily.rio.reproject_match(et_tseries[0], resampling = Resampling.bilinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting ecostress availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_tseries = et_tseries.sel(band=1)\n",
    "\n",
    "et_2018_may_sept=et_tseries.sel(date=slice(\"2018-06-01\", \"2018-09-30\"))\n",
    "\n",
    "et_2018_may_sept=et_2018_may_sept.chunk(chunks={\"date\": 101, \"y\": 1000, \"x\": 1000})\n",
    "\n",
    "et_2018_may_sept = et_2018_may_sept.where(et_2018_may_sept > 0)\n",
    "\n",
    "et_nonnan_count = ~np.isnan(et_2018_may_sept)\n",
    "\n",
    "true_count = et_nonnan_count.astype(bool).sum(dim=\"date\")\n",
    "\n",
    "true_count_c = true_count.compute()\n",
    "\n",
    "f, ax = plt.subplots(1)\n",
    "true_count_c.plot.imshow(ax=ax)\n",
    "plt.title(\"Number of ECOSTRESS Observations, May-Sept 2018\")\n",
    "france_river_lines.plot(ax=ax, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating hourly data to dekad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pev_dekad = met_dataset['pev'].resample(time='1D').sum().resample(time='10D').mean()\n",
    "\n",
    "pev_dekad_2018 = pev_dekad.sel(time=slice(\"2018-01-01\", \"2018-10-01\"))\n",
    "\n",
    "met_dataset['pev'].sel(time=\"2019-04-01\")\n",
    "\n",
    "pev_dekad_2019.plot(x='longitude', y='latitude', col='time', col_wrap=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretty ET plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ETcolors = [\"#f6e8c3\", \"#d8b365\", \"#99974a\", \"#53792d\", \"#6bdfd2\", \"#1839c5\"]\n",
    "ETcmap = LinearSegmentedColormap.from_list(\"ET\", ETcolors)\n",
    "date_utc = pd.to_datetime(et['date'].values)\n",
    "layer_type = et.attrs['filename'].split(\"_\")[-3]\n",
    "title = 'ECO3ETPTJPL Evapotranspiration'\n",
    "\n",
    "fig = plt.figure(figsize=(9.7,7.6))                                                       # Set the figure size (x,y)\n",
    "fig.suptitle(f'{title} ({layer_type}) \\n at {date_utc}', fontsize=22)  # Add title for the plots\n",
    "plt.axis('off')                                                                           # Remove axes from plot\n",
    "im = plt.imshow(et.sel(band=1), cmap=ETcmap);                                                        # Plot array using colormap\n",
    "# plt.scatter(Tcol, Trow, color=\"black\", marker='x')                                        # Plot tower location\n",
    "# Add a colormap legend\n",
    "plt.colorbar(im, orientation='horizontal', fraction=0.05, pad=0.004, label=f\"ET ({et.attrs['units']})\", shrink=0.6).outline.set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to plot xarray image data and geopandas data with ipyleaflet but it can't do image overlays yet (unless it comes from a server via url potentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyleaflet as ipyl\n",
    "x = france_river_lines.unary_union.envelope.centroid.xy[0][0]\n",
    "y = france_river_lines.unary_union.envelope.centroid.xy[1][0]\n",
    "e, n, w, s =true_count_c.rio.bounds()\n",
    "m = ipyl.Map(center = (y,x), zoom=6)\n",
    "\n",
    "rivers_data = ipyl.GeoData(geo_dataframe = france_river_lines,\n",
    "                   style={'color': 'purple', 'opacity':3, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n",
    "                   hover_style={'fillColor': 'red' , 'fillOpacity': 0.2},\n",
    "                   name = 'Rivers')\n",
    "m.add_layer(rivers_data)\n",
    "plt.imsave(\"observation_count.jpeg\",true_count_c)\n",
    "obs_heatmap = ipyl.ImageOverlay(\n",
    "    url=\"observation_count.jpeg\",\n",
    "    bounds=((s, w), (n, e))\n",
    ")\n",
    "\n",
    "m.add_layer(obs_heatmap)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to plot ecostress DataArray with geopandas shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartopy_project_geo_df(df, crs):\n",
    "\n",
    "    # This can be converted into a `proj4` string/dict compatible with GeoPandas\n",
    "    crs_proj4 = crs.proj4_init\n",
    "    return df.to_crs(crs_proj4)\n",
    "\n",
    "crs = ccrs.PlateCarree()\n",
    "aoi_projected = cartopy_project_geo_df(aoi, crs)\n",
    "france_rivers_df_projected = cartopy_project_geo_df(france_rivers_df, crs)\n",
    "# base = aoi_projected.plot(color=\"grey\", edgecolor=\"black\")\n",
    "# france_rivers_df_projected.plot(ax=base, color=\"blue\")\n",
    "\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes(projection=crs)\n",
    "all_et_daily[14].sel(band=1).plot.imshow(ax=ax, transform=crs)\n",
    "# ax.add_geometries(aoi_projected['geometry'], crs=crs)\n",
    "ax.add_geometries(france_rivers_df_projected['geometry'], crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_NA_values():\n",
    "    \"\"\"\n",
    "    Daily ET products have both nan values from where there are clouds \n",
    "    and -1e+13 for where the ecostress swath was clipped during the ordering process\n",
    "    \"\"\"\n",
    "    masked_et = np.ma.masked_where(et.sel(band=1) == np.nan, et.sel(band=1))\n",
    "    masked_et = np.ma.masked_where(masked_et == -1e+13, masked_et)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
